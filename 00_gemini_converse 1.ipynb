{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesi√≥n con Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este bloque instala las bibliotecas necesarias para trabajar con la API de Gemini y con PDFs\n",
    "# Nota: Este comando solo necesita ejecutarse una vez\n",
    "!pip install google-generativeai\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n API Key & Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zona para introducir tu clave API:\n",
    "# ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è\n",
    "API_KEY = \"REEMPLAZA_ESTO_CON_TU_CLAVE_API_REAL\"\n",
    "# ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=API_KEY)\n",
    "modelo = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "chat = modelo.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_respuesta(respuesta, titulo=\"Respuesta:\"):\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"‚ú® {titulo} ‚ú®\")\n",
    "    print(\"=\" * 50)\n",
    "    display(Markdown(respuesta.text))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversar(mensaje):\n",
    "    print(f\"\\nüìù T√∫: {mensaje}\")\n",
    "    respuesta = chat.send_message(mensaje)\n",
    "    print(f\"\\nü§ñ Gemini: {respuesta.text}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqu√≠ aprendemos a configurar los par√°metros que controlan c√≥mo responde el modelo\n",
    "\n",
    "# Configuramos par√°metros de generaci√≥n personalizados\n",
    "config_creativa = genai.GenerationConfig(\n",
    "    temperature=0.9,           # Alta temperatura = m√°s creativo\n",
    "    max_output_tokens=500,     # Limita la longitud de la respuesta\n",
    ")\n",
    "\n",
    "config_precisa = genai.GenerationConfig(\n",
    "    temperature=0.1,           # Baja temperatura = m√°s preciso/determinista\n",
    "    max_output_tokens=500,\n",
    ")\n",
    "\n",
    "\n",
    "# Ejemplo de uso de diferentes configuraciones\n",
    "pregunta = \"Escribe un poema corto sobre la programaci√≥n\"\n",
    "\n",
    "print(\"\\nüîç COMPARACI√ìN DE CONFIGURACIONES:\")\n",
    "print(\"Misma pregunta, diferentes configuraciones de generaci√≥n\")\n",
    "\n",
    "# Respuesta con configuraci√≥n creativa\n",
    "respuesta_creativa = modelo.generate_content(pregunta, generation_config=config_creativa)\n",
    "mostrar_respuesta(respuesta_creativa, f\"Respuesta CREATIVA (temperature={config_creativa.temperature})\")\n",
    "\n",
    "# Respuesta con configuraci√≥n precisa\n",
    "respuesta_precisa = modelo.generate_content(pregunta, generation_config=config_precisa)\n",
    "mostrar_respuesta(respuesta_precisa, f\"Respuesta PRECISA (temperature={config_precisa.temperature})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat multiturno con memoria de conversaci√≥n\n",
    "\n",
    "# Iniciamos una sesi√≥n de chat\n",
    "chat = modelo.start_chat(history=[])\n",
    "print(\"\\nEjemplo de conversaci√≥n con memoria:\")\n",
    "\n",
    "# Primera pregunta\n",
    "conversar(\"¬øQu√© es Python?\")\n",
    "\n",
    "# Segunda pregunta (se refiere a la primera sin mencionarla expl√≠citamente)\n",
    "conversar(\"¬øCu√°les son sus principales ventajas?\")\n",
    "\n",
    "# Tercera pregunta (continuando la conversaci√≥n)\n",
    "conversar(\"Dame un ejemplo de c√≥digo simple\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
